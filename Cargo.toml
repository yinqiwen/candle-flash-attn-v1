[package]
name = "candle-flash-attn-v1"
version = "0.0.1"
edition = "2021"

description = "Flash attention V1 layer for the candle ML framework."
keywords = ["blas", "tensor", "machine-learning"]
categories = ["science"]
license = "MIT OR Apache-2.0"
readme = "README.md"

[dependencies]
# candle = { version = "*", package = "candle-core", features = ["cuda"] }
candle = { package = "candle-core", git = "https://github.com/yinqiwen/candle", branch = "main", features = [
    "cuda",
] }
half = { version = "2.3.1", features = ["num-traits"] }

[build-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
num_cpus = "1.15.0"
rayon = "1.7.0"

[dev-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
candle-nn = { version = "0.3.0", features = ["cuda"] }
